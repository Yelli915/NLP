# #bpe ì•„ì´ë””ì–´ ê¸°ë°˜ vocab ìƒì„±.
from tokenizer import CustomTokenizer

# corpus = [
#     "ë‚˜ëŠ” í•™êµì— ê°„ë‹¤",
#     "í•™êµì— ë‹¤ì‹œ ê°„ë‹¤",
#     "ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì¢‹ë‹¤",
#     "ë‚˜ëŠ” ì˜¤ëŠ˜ í•™êµì— ê°”ë‹¤"
# ]

# tokenizer = CustomTokenizer(vocab_size=50, tokenizer_type='bpe')
# tokenizer.train_bpe(corpus)

# for token, idx in tokenizer.vocab.items():
#     print(f"{idx:>2} : {token}")


# tok = tokenizer.tokenize("í•™êµì— ë‹¤ì‹œ ê°„ë‹¤.ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì¢‹ë‹¤.")
# print(tok)

from tokenizer import CustomTokenizer

def test_tokenizer(tokenizer_type):
    print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¤‘: {tokenizer_type.upper()} ë°©ì‹")
    
    corpus = [
        "ë‚˜ëŠ” í•™êµì— ê°”ë‹¤.",
        "ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì¢‹ë‹¤.",
        "í•™êµì— ë‹¤ì‹œ ê°”ë‹¤.",
        "ë‚˜ëŠ” ì˜¤ëŠ˜ í•™êµì— ê°”ë‹¤."
    ]
    
    test_text = "ë‚˜ëŠ” ì˜¤ëŠ˜ í•™êµì— ê°”ë‹¤."
    
    tokenizer = CustomTokenizer(vocab_size=50, tokenizer_type=tokenizer_type)
    
    if tokenizer_type == 'freq':
        tokenizer.train_freq(corpus)
    elif tokenizer_type == 'okt':
        tokenizer.train_okt(corpus)
    elif tokenizer_type == 'bpe':
        tokenizer.train_bpe(corpus)
    
    print("ğŸ“˜ Vocab:")
    for token, idx in tokenizer.vocab.items():
        print(f"{idx:>2} : {token}")
    
    input_ids, attention_mask, token_type_ids = tokenizer.encode(test_text, max_length=20)
    print("\nâœ… encode ê²°ê³¼:")
    print("input_ids      :", input_ids)
    print("attention_mask :", attention_mask)
    print("token_type_ids :", token_type_ids)
    
    inputs = tokenizer(test_text, max_length=20, return_tensors='pt')
    print("âœ… __call__() ê²°ê³¼:")
    for k, v in inputs.items():
        print(f"{k:15}: {v}")
    
    print("âœ… decode ê²°ê³¼:")
    print(tokenizer.decode(input_ids))

# ì„¸ ë°©ì‹ ëª¨ë‘ ì‹¤í–‰
for mode in ['freq', 'okt', 'bpe']:
    test_tokenizer(mode)

